import xml.etree.ElementTree as ET
import urllib.request
import sys
import os
import time 
from datetime import datetime
import sys

rss_feeds = []

assert len(sys.argv) <= 3, "Too many arguments"

if len(sys.argv) == 1:
	rss_path = "links.txt"
else:
	rss_path = sys.argv[1]

if len(sys.argv) <= 2:
	res_folder = "res"
else:
	res_folder = sys.argv[2]

cache_path = res_folder + "/cache.txt"
cache_expiry = 604800 # One week in seconds
cache = []
links = []

current_time = int(time.time())
current_date = datetime.today().strftime('%Y-%m-%d')

headers = {"user-agent": "Mozilla/5.0"}

warning = lambda text: print("--> Warning:",text)
error = lambda errtype,text: print("--> ERROR ("+ errtype +"):",text)

def load_rss_feeds ():
	with open (rss_path,"r") as rss_list_file:
		for line in rss_list_file.read().split("\n"):
			line = line.rstrip()
			if len(line) > 1 and line[0] != "#":
				arr = line.split("\t")
				if len(arr) == 1: rss_feeds.append([None,arr[0]])
				elif len(arr) == 2: rss_feeds.append(arr)
				else: raise ValueError ("Too many tabs in line that contains \"" + line + "\"")

def load_cache ():
	if os.path.isdir(res_folder):
		if os.path.isfile(cache_path):
			with open(cache_path,"r") as cache_file:
				for line in cache_file.read().split("\n")[2:]:
					line_list = line.split("\t")
					if len(line_list) == 2:
						line_list[0] = int(line_list[0])
						if current_time - line_list[0] < cache_expiry:
							cache.append(line_list)
							links.append(line_list[1])
	else:
		os.mkdir(res_folder)

def unload_cache ():
	cache_string = "# This is a file that stores links to all articles accessed for a week to avoid repetition\n"
	cache_string += "# Please do not edit this file in any way\n"
	if not os.path.isdir(res_folder): os.mkdir(res_folder)

	for time,link in cache: cache_string += str(time) + "\t" + link + "\n"
	with open(cache_path,"w") as cache_file: 
		cache_file.write(cache_string)

def access_page (url,content_type_wanted,werr = False):
	req = urllib.request.Request(url, headers = headers)
	try:
		with urllib.request.urlopen(req) as page:
			if page.status < 400:
				raw_text = page.read()
				content_type = page.getheader("Content-Type")
				if content_type_wanted not in content_type:
					if not werr: warning("Warning: Webpage may not be properly formatted")
					else: return None
			else:
				error("HTTP Error",page.status)
				return None
	
	except urllib.error.URLError as e:
		error ("URLError",e.reason)
		return None
	
	return raw_text

def file_title (title, feed_title):
	# Characters not allowed in Windows filenames
	replace = [	("/"," "), ("\\"," "), (">"," "), ('"',"'"), (":","- "),
					("*"," "), ("|","- "), ("<"," "), ("?","") ]
		
	for old,new in replace:
		title = title.replace(old,new)

	folder = res_folder + "/" + current_date + "/" + feed_title
	if not os.path.isdir(folder): os.makedirs(folder)
	return folder + "/" + title + ".html"

def do_item (item,feed_title):
	title = item.find("title")
	link = item.find("link")
	if not (title is None or link is None) and link.text not in links:
		print("Saving Article:",title.text)
		text = access_page(link.text,"text/html", werr = True)
		cache.append([current_time,link.text])
		if text is not None:
			with open(file_title(title.text,feed_title),"wb") as file:
				file.write(text)
			return True

	return False
		
def find_links (root, feed_url, title):
	if title is None: 
		title = root.find("title")
		if title is None: title = feed_url
		else: title = title.text

	print("Getting articles from",title)
	count = 0
	for item in root.findall("item"):
		count += int(do_item(item,title))
	print (count,"new articles fetched in \n")
	return count

# MAIN
load_rss_feeds()
load_cache()
for feed_title, feed_url in rss_feeds:
	if feed_title is None: print("Fetching RSS feed",feed_url)
	else: print("Fetching RSS feed",feed_title)

	raw_text = access_page(feed_url,"text/xml")
	if raw_text is not None:
		find_links(ET.fromstring(raw_text).find("channel"),feed_url,feed_title)
unload_cache()
